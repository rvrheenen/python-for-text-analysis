{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3a: Revision of block 3\n",
    "\n",
    "## Due: Friday the 27th of September 2019 23:59 p.m.\n",
    "\n",
    "* Please submit your assignment (notebooks of parts 3a and 3b + Python modules) as **a single .zip file** using [this google form](https://forms.gle/JiDmuLLKxbjgA8Sn8)\n",
    "\n",
    "* Please name your zip file with the following naming convention: ASSIGNMENT_3_FIRSTNAME_LASTNAME.zip\n",
    "\n",
    "If you have **questions** about this topic, please contact **Marten (m.c.postma@vu.nl)**.\n",
    "\n",
    "\n",
    "In this block, we covered a lot of ground:\n",
    "\n",
    "* Chapter 12 - Importing external modules \n",
    "* Chapter 13 - Working with Python scripts\n",
    "* Chapter 14 - Reading and writing text files\n",
    "* Chapter 15 - Off to analyzing text \n",
    "\n",
    "\n",
    "In this assignment, you will first complete a number of small exercises about each chapter to make sure you are familiar with the most important concepts. In the second part of the assignment, you will apply your newly acquired skills to write your very own text processing program (ASSIGNMENT-3b) :-). But don't worry, there will be instructions and hints along the way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions & scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 1:\n",
    "\n",
    "Define a function called `split_sort_text` which takes one positional parameter called **text** (a string).\n",
    "\n",
    "The function:\n",
    "* splits the string on a space characters, i.e., ' '\n",
    "* returns all the unique words in alphabetical order as a list.\n",
    "\n",
    "* Hint 1: There is a specific python container which does not allow for duplicates and simply removes them. Use this one. \n",
    "* Hint 2: There is a function which sorts items in an iterable called 'sorted'. Look at the documentation to see how it is used. \n",
    "* Hint 3: Don't forget to write a docstring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'test', 'that', 'the', 'this']\n"
     ]
    }
   ],
   "source": [
    "def split_sort_text(text):\n",
    "    '''Returns all the unique words in alphabetical order as a list'''\n",
    "    return sorted(list(set(text.split())))\n",
    "\n",
    "print(split_sort_text(\"this test is the test that this is\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with external modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "NLTK offers a way of using WordNet in Python. Do some research (using google, because quite frankly, that's what we do very often) and see if you can find out how to import it. WordNet is a computational lexicon which organizes words according to their senses (collected in synsets). See if you can print all the synset definitions (i.e. entries) of the word 'dog'.\n",
    "\n",
    "Make sure you have run the following cell to make sure you have installed WordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/rick/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# uncomment the following line to download material including WordNet\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('dog.n.01')\n",
      "Synset('frump.n.01')\n",
      "Synset('dog.n.03')\n",
      "Synset('cad.n.01')\n",
      "Synset('frank.n.02')\n",
      "Synset('pawl.n.01')\n",
      "Synset('andiron.n.01')\n",
      "Synset('chase.v.01')\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "print(\"\\n\".join([str(thing) for thing in wn.synsets('dog')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with python scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise  3\n",
    "\n",
    "\n",
    "#### a.) Define a function called `count` which counts the words in a string. Do not use NLTK just yet. Find a way to test it.  \n",
    "* Hint 1: Write a helper-function called `preprocess` which preprocesses the string (split it, remove the punctuation specified by the user, return it in a container that you think works best for the next steps). You call the function `preprocess` inside the `count` function.\n",
    "\n",
    "* Hint 2: Remember that there are string methods which you can use to get rid of unwanted characters. Test the `preprocess` function using the string 'this is a (tricky) test'. No assert statements are needed.\n",
    "\n",
    "* Tip 3: Remember how we used dictionaries to count words? If not, have a look at the containers chapter. \n",
    "\n",
    "* Hint 4: Test your function using an example string which will tell you whether it fullfils the requirements (remove punctuation, split, count). You will get a point for good testing.\n",
    "\n",
    "#### b.) Create a python script \n",
    "\n",
    "Use your editor to create a python script called `count_words.py`. Move your function call of the **count** function to this file. Move your helper function (**preprocess**) to a seperate script which you call `utils_3a.py`. Import your helper function into `count_words.py`. Test whether everything works as expected by calling the scipt `count_words.py` from the terminal. \n",
    "\n",
    "**Please submit these scripts together with the other notebooks**.\n",
    "\n",
    "Don't forget to add docstrings to your functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': 1, 'tricky': 1, 'this': 2, 'a': 1, 'is': 2}\n"
     ]
    }
   ],
   "source": [
    "# Was it on purpose that Tip 3 was a tip and not a hint? 🤔\n",
    "from count_words import count\n",
    "\n",
    "print(count(\"this is a (tricky) test, is this?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "**Playing with lyrics**\n",
    "\n",
    "\n",
    "a.) Write a function called `load_text` which opens and reads a file and returns the text in the file. It should take a filepath as a argument. Test it by loading this file: ../Data/lyrics/walrus.txt\n",
    "\n",
    "* Hint: remember it is best practice to use a context manager\n",
    "\n",
    "b.) Write a function called `replace_walrus`  which takes lyrics as input and replaces every instance of 'walrus' by 'hippo' (make sure to account for upper and lower case - it is fine to transform everything to lower case). The function should write the new version of the song to a file called 'walrus_hippo.txt and stored in ../Data/lyrics. \n",
    "\n",
    "Don't forget to add docstrings to your functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I Am The Walrus\"\n",
      "(\"Magical Mystery Tour\" Version)\n",
      "\n",
      "I am he\n",
      "As you are he\n",
      "As you are me\n",
      "And we are all together\n",
      "\n",
      "See how they run\n",
      "Like pigs from a gun\n",
      "See how they fly\n",
      "I'm crying\n",
      "\n",
      "Sitting on a cornflake\n",
      "Waiting for the van to come\n",
      "Corporation tee shirt\n",
      "Stupid bloody Tuesday\n",
      "Man, you been a naughty boy\n",
      "You let your face grow long\n",
      "\n",
      "I am the eggman (Ooh)\n",
      "They are the eggmen, (Ooh)\n",
      "I am the walrus\n",
      "Goo goo g' joob\n",
      "\n",
      "Mister city p'liceman sitting pretty\n",
      "Little p'licemen in a row\n",
      "See how they fly\n",
      "Like Lucy in the sky\n",
      "See how they run\n",
      "I'm crying\n",
      "I'm crying, I'm crying, I'm crying\n",
      "\n",
      "Yellow matter custard\n",
      "Dripping from a dead dog's eye\n",
      "Crabalocker fishwife pornographic priestess\n",
      "Boy you been a naughty girl\n",
      "You let your knickers down\n",
      "\n",
      "I am the eggman (Ooh)\n",
      "They are the eggmen (Ooh)\n",
      "I am the walrus\n",
      "Goo goo g' joob\n",
      "\n",
      "Sitting in an English\n",
      "Garden waiting for the sun\n",
      "If the sun don't come\n",
      "You get a tan from standing in the English rain\n",
      "\n",
      "I am the eggman\n",
      "They are the eggmen\n",
      "I am the walrus\n",
      "Goo goo g' joob g' goo goo g' joob\n",
      "\n",
      "Expert texpert choking smokers\n",
      "Don't you think the joker laughs at you?\n",
      "See how they smile\n",
      "Like pigs in a sty, see how they snied\n",
      "I'm crying\n",
      "\n",
      "Semolina pilchards\n",
      "Climbing up the Eiffel Tower\n",
      "Element'ry penguin singing Hare Krishna\n",
      "Man, you should have seen them kicking Edgar Allan Poe\n",
      "\n",
      "I am the eggman (Ooh)\n",
      "They are the eggmen (Ooh)\n",
      "I am the walrus\n",
      "Goo goo g' joob\n",
      "Goo goo g' joob\n",
      "G' goo goo g' joob\n",
      "Goo goo g' joob, goo goo g' goo g' goo goo g' joob joob\n",
      "Joob joob...\n"
     ]
    }
   ],
   "source": [
    "def load_text(filepath):\n",
    "    with open(filepath) as f:    \n",
    "        data = f.read() \n",
    "    return data\n",
    "print(load_text(\"../Data/lyrics/walrus.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing text with nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "**Building a simple NLP pipeline**\n",
    "\n",
    "For this exercise, you will need NLTK. Don't forget to import it. \n",
    "\n",
    "Write a function called `tag_text` which takes raw text as input and returns the tagged text. To do this, make sure you follow the steps below:\n",
    "\n",
    "* Tokenize the text. \n",
    "\n",
    "* Perform part-of-speech tagging on the list of tokens. \n",
    "\n",
    "* Return the tagged text\n",
    "\n",
    "\n",
    "Then test your function using the text snipped below (`test_text`) as input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"Two households, both alike in dignity,\n",
    "    In fair Verona, where we lay our scene,\n",
    "    From ancient grudge break to new mutiny,\n",
    "    Where civil blood makes civil hands unclean.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Two', 'CD'), ('households', 'NNS'), (',', ','), ('both', 'DT'), ('alike', 'RB'), ('in', 'IN'), ('dignity', 'NN'), (',', ','), ('In', 'IN'), ('fair', 'JJ'), ('Verona', 'NNP'), (',', ','), ('where', 'WRB'), ('we', 'PRP'), ('lay', 'VBD'), ('our', 'PRP$'), ('scene', 'NN'), (',', ','), ('From', 'NNP'), ('ancient', 'NN'), ('grudge', 'NN'), ('break', 'NN'), ('to', 'TO'), ('new', 'JJ'), ('mutiny', 'NN'), (',', ','), ('Where', 'NNP'), ('civil', 'JJ'), ('blood', 'NN'), ('makes', 'VBZ'), ('civil', 'JJ'), ('hands', 'NNS'), ('unclean', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def tag_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "print(tag_text(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "6.a) How many for-loops can you nest in one another? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When generating nested loops, Python raised an error on the 21th nested loop.\n",
      "Thus the amount of for-loops you can nest is 20.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for n in range(1000):\n",
    "        exec('\\n'.join([(' '*i) + f'for i{i} in range(1):' for i in range(n)])+ '\\n' + ' ' * n + 'pass\\n')\n",
    "except SyntaxError as e:\n",
    "    print(f\"When generating nested loops, Python raised an error on the {n}th nested loop.\\nThus the amount of for-loops you can nest is {n-1}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.b) What is the difference between the modes 'w' and 'a' when opening a file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When opening a file 'w' means write, where 'a' means append, so the write will clear the contents while append will add to it.\n"
     ]
    }
   ],
   "source": [
    "print(\"When opening a file 'w' means write, where 'a' means append, so the write will clear the contents while append will add to it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
