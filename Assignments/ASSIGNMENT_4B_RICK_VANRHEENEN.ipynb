{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4b: Build your own corpus exploration tool\n",
    "\n",
    "**Deadline for Assignment 4a+b: Tuesday, October 8, 2019 (20.00).** \n",
    "\n",
    "Please submit the assignment using this [this google form](https://forms.gle/qbBWiDh55oMozBJQ9) using the guidelines specified in Assignment 4a. \n",
    "\n",
    "In this assignment, you're going to build your own tool for exploring a the **Parallel Meaning Bank** (PMB). This resource is a **parallel corpus**, which means that it contains the **same documents in multiple languages**. Such resources are very valuable for many aspects of linguistics and Natural Lanugage Processing (NLP), but most importantly for Machine Translation (ML). \n",
    "\n",
    "For this part of assignment 4, you will submit two python scripts called:\n",
    "\n",
    "* `explore_pmb.py`\n",
    "* `utils.py`\n",
    "\n",
    "Your tool will be able to:\n",
    "\n",
    "* explore the **overall coverage per language**\n",
    "* explore the the **parallel coverage of a given language pair** (i.e. how many documents and tokens exist in a language pair?)\n",
    "* **browse parallel text** in given language pairs\n",
    "\n",
    "Before diving into building the tool, we're going to guide you through a couple of warm-up examples. You can use them to explore the data structure and write your code. It is permitted to copy-paste bits of code (you will have to modify them to solve all exercises). \n",
    "\n",
    "The assignment is structured as follows:\n",
    "\n",
    "1. Understanding the data structure (code snippets to guide you through the corpus)\n",
    "2. Writing functions (writing the actual code)\n",
    "3. Putting the tool together (combining the code)\n",
    "4. Testing and submission (a final check of whether your code does what it is supposed to do)\n",
    "\n",
    "\n",
    "You can learn more about the PMB [here](https://pmb.let.rug.nl/). \n",
    "\n",
    "If you have any questions about this assignment, please contact **Pia Sommerauer (pia.sommerauer@vu.nl)**. Questions and answers will be collected in [this Q&A document](https://docs.google.com/document/d/1ebXRAEzkcBwowpcYf-6n_FM3UdwpZCXOvhH8Kd0bAfU/edit?usp=sharing), so please check it before you email. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the data structure\n",
    "\n",
    "In this part, we guide you through the data structure. You can use the code below for the rest of your assignment. You can play with the code and add things to it, but you will not receive points in this part. Its purpose is to make you familiar with the data structure.  \n",
    "\n",
    "### 1.a Download the data\n",
    "\n",
    "1.) Please go to this website: [here](https://pmb.let.rug.nl/data.php)\n",
    "\n",
    "2.) Select version 2.1.0 (the latest version is too big for our purposes) and store the zip file as `PMB/pmb-2.1.0.zip`. \n",
    "\n",
    "3.) Unpack the data. You can do this from the terminal by navigating to the directory using `cd`. You should be able to run `unzip pmb-2.1.0.zip` to unzip the file. Alternatively, you can simply unzip by clicking on it. Attention: Unpacking the file may take a while. \n",
    "\n",
    "Use the cell below to assign the path to the data to a variable. We will only consider the gold data for this assignment, therefore you can add the gold directory to the path.\n",
    "\n",
    "Path: `'PMB/pmb-2.1.0/data/gold/'`\n",
    "\n",
    "**Please run the following cell to check if your data are in the right place. If they are, it will not print anything.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path_pmb = 'PMB/pmb-2.1.0/data/gold'\n",
    "assert os.path.isdir(path_pmb), 'corpus data not found'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b Parallel documents \n",
    "Before we can build anything, we have to understand how the data are strucutred. We start by looking at a single document. \n",
    "\n",
    "Parallel documents are stored in the same document directory (d+number). The filenames indicate the language (e.g. en = English). The data we're interested in are stored in .xml format. Run the cell below to inspect the filepaths of a single document. Feel free to modify the path to inspect other documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMB/pmb-2.1.0/data/gold/p27/d0852/en.drs.xml\n",
      "PMB/pmb-2.1.0/data/gold/p27/d0852/de.drs.xml\n",
      "PMB/pmb-2.1.0/data/gold/p27/d0852/nl.drs.xml\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "test_part = 'p27'\n",
    "test_document = 'd0852'\n",
    "\n",
    "test_doc_path = f'{path_pmb}/{test_part}/{test_document}/'\n",
    "test_doc_files = glob.glob(f'{test_doc_path}*.xml')\n",
    "\n",
    "for f in test_doc_files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c XML structure of a single document\n",
    "\n",
    "Below, we access a single document and load the xml structure using lxml.etree. Run the cell to print the xml tree. \n",
    "\n",
    "Explore the document structure and try to answer these questions:\n",
    "\n",
    "* Where can you find the full text of the document?\n",
    "* Where can you find information about each token in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xdrs-output version=\"'boxer v1.00 (unix build on 24 May 2018, 11:14:54)'\">\n",
      "<!-- I 'm not tired at~all . --> \n",
      "\n",
      "<xdrs xml:id=\"xdrs1\">\n",
      " <taggedtokens>\n",
      "  <tagtoken xml:id=\"i1001\">\n",
      "   <tags>\n",
      "     <tag type=\"verbnet\" n=\"0\">[]</tag>\n",
      "     <tag type=\"tok\">I</tag>\n",
      "     <tag type=\"sym\">speaker</tag>\n",
      "     <tag type=\"lemma\">speaker</tag>\n",
      "     <tag type=\"from\">0</tag>\n",
      "     <tag type=\"to\">1</tag>\n",
      "     <tag type=\"pos\">PRP</tag>\n",
      "     <tag type=\"sem\">PRO</tag>\n",
      "     <tag type=\"wordnet\">O</tag>\n",
      "   </tags>\n",
      "  </tagtoken>\n",
      "  <tagtoken xml:id=\"i1002\">\n",
      "   <tags>\n",
      "     <tag type=\"verbnet\" n=\"0\">[]</tag>\n",
      "     <tag type=\"tok\">'m</tag>\n",
      "     <tag type=\"sym\">be</tag>\n",
      "     <tag type=\"lemma\">be</tag>\n",
      "     <tag type=\"from\">1</tag>\n",
      "     <tag type=\"to\">3</tag>\n",
      "     <tag type=\"pos\">VBP</tag>\n",
      "     <tag type=\"sem\">NOW</tag>\n",
      "     <tag type=\"wordnet\">O</tag>\n",
      "   </tags>\n",
      "  </tagtoken>\n",
      "  <tagtoken xml:id=\"i1003\">\n",
      "   <tags>\n",
      "     <tag type=\"verbnet\" n=\"0\">[]</tag>\n",
      "     <tag type=\"tok\">not</tag>\n",
      "     <tag type=\"sym\">not</tag>\n",
      "     <tag type=\"lemma\">not</tag>\n",
      "     <tag type=\"from\">4</tag>\n",
      "     <tag type=\"to\">7</tag>\n",
      "     <tag type=\"pos\">RB</tag>\n",
      "     <tag type=\"sem\">NOT</tag>\n",
      "     <tag type=\"wordnet\">O</tag>\n",
      "   </tags>\n",
      "  </tagtoken>\n",
      "  <tagtoken xml:id=\"i1004\">\n",
      "   <tags>\n",
      "     <tag type=\"verbnet\" n=\"0\">[]</tag>\n",
      "     <tag type=\"tok\">tired</tag>\n",
      "     <tag type=\"sym\">tired</tag>\n",
      "     <tag type=\"lemma\">tired</tag>\n",
      "     <tag type=\"from\">8</tag>\n",
      "     <tag type=\"to\">13</tag>\n",
      "     <tag type=\"pos\">JJ</tag>\n",
      "     <tag type=\"sem\">IST</tag>\n",
      "     <tag type=\"wordnet\">tired.a.01</tag>\n",
      "   </tags>\n",
      "  </tagtoken>\n",
      "  <tagtoken xml:id=\"i1005\">\n",
      "   <tags>\n",
      "     <tag type=\"verbnet\" n=\"0\">[]</tag>\n",
      "     <tag type=\"tok\">at~all</tag>\n",
      "     <tag type=\"sym\">at~all</tag>\n",
      "     <tag type=\"lemma\">at~all</tag>\n",
      "     <tag type=\"from\">14</tag>\n",
      "     <tag type=\"to\">20</tag>\n",
      "     <tag type=\"pos\">NN</tag>\n",
      "     <tag type=\"sem\">EMP</tag>\n",
      "     <tag type=\"wordnet\">O</tag>\n",
      "   </tags>\n",
      "  </tagtoken>\n",
      "  <tagtoken xml:id=\"i1006\">\n",
      "   <tags>\n",
      "     <tag type=\"verbnet\" n=\"0\">[]</tag>\n",
      "     <tag type=\"tok\">.</tag>\n",
      "     <tag type=\"sym\">.</tag>\n",
      "     <tag type=\"lemma\">.</tag>\n",
      "     <tag type=\"from\">20</tag>\n",
      "     <tag type=\"to\">21</tag>\n",
      "     <tag type=\"pos\">.</tag>\n",
      "     <tag type=\"sem\">NIL</tag>\n",
      "     <tag type=\"wordnet\">O</tag>\n",
      "   </tags>\n",
      "  </tagtoken>\n",
      " </taggedtokens>\n",
      " <drs type=\"sentence\" label=\"b1\">\n",
      "  <tokens>\n",
      "  </tokens>\n",
      "  <domain>\n",
      "  </domain>\n",
      "  <conds>\n",
      "   <cond label=\"b1\">\n",
      "    <not from=\"4\" to=\"7\">\n",
      "     <drs type=\"normal\" label=\"b2\">\n",
      "      <domain>\n",
      "       <dr label=\"b2\" name=\"s1\" from=\"8\" to=\"13\"/>\n",
      "       <dr label=\"b3\" name=\"t1\" from=\"1\" to=\"3\"/>\n",
      "      </domain>\n",
      "      <conds>\n",
      "       <cond label=\"b2\">\n",
      "        <pred arg=\"s1\" symbol=\"tired\" synset=\"tired.a.01\" from=\"8\" to=\"13\"/>\n",
      "       </cond>\n",
      "       <cond label=\"b2\">\n",
      "        <rel arg1=\"s1\" arg2=\"t1\" symbol=\"Time\" from=\"1\" to=\"3\"/>\n",
      "       </cond>\n",
      "       <cond label=\"b2\">\n",
      "        <rel arg1=\"s1\" arg2=\"speaker\" symbol=\"Theme\" from=\"8\" to=\"13\"/>\n",
      "       </cond>\n",
      "       <cond label=\"b3\">\n",
      "        <pred arg=\"t1\" symbol=\"time\" synset=\"time.n.08\" from=\"1\" to=\"3\"/>\n",
      "       </cond>\n",
      "       <cond label=\"b3\">\n",
      "        <comp symbol=\"EQU\" arg1=\"t1\" arg2=\"now\" from=\"1\" to=\"3\"/>\n",
      "       </cond>\n",
      "      </conds>\n",
      "     </drs>\n",
      "    </not>\n",
      "   </cond>\n",
      "  </conds>\n",
      " </drs>\n",
      "</xdrs>\n",
      "</xdrs-output>\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "\n",
    "test_doc_path_en = test_doc_path+'en.drs.xml'\n",
    "doc_tree = etree.parse(test_doc_path_en)\n",
    "doc_root = doc_tree.getroot()\n",
    "etree.dump(doc_root, pretty_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing functions\n",
    "\n",
    "In this part of the assigment, we guide you through writing the functions for your tool. Feel free to use the notebook for exploration, but your final functions should be stored in `utils.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Get all token elements of a document in a given language\n",
    "\n",
    "Write a function which fulfills the following requirements: \n",
    "\n",
    "* Mandatory ositional argument: path to the document in a particular lanugage \n",
    "* Output: list of token elements (the token elements are called 'tagtoken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(path_to_doc):\n",
    "    return etree.parse(path_to_doc).findall(\"xdrs/taggedtokens/tagtoken\")\n",
    "    \n",
    "\n",
    "# Test you function\n",
    "test_part = 'p27'\n",
    "test_document = 'd0852'\n",
    "language = 'en'\n",
    "test_doc_path = f'{path_pmb}/{test_part}/{test_document}/{language}.drs.xml'\n",
    "\n",
    "# Function call\n",
    "tokens = get_tokens(test_doc_path)\n",
    "\n",
    "assert len(tokens) == 6 and type(tokens[1]) == etree._Element, 'token list not correct'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b Get token and pos from a token element\n",
    "\n",
    "Write a function which fulfills the following requirements: \n",
    "\n",
    "* Mandatory positional argument: token element\n",
    "* Output: token (string) and part of speech tag (string) of the token element\n",
    "\n",
    "An example token element is shown below. (You can use it for testing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element tagtoken at 0x10b0376c8>\n"
     ]
    }
   ],
   "source": [
    "test_token_str = \"\"\"\n",
    " <tagtoken xml:id=\"i1002\">\n",
    "   <tags>\n",
    "     <tag type=\"verbnet\" n=\"0\">[]</tag>\n",
    "     <tag type=\"tok\">'m</tag>\n",
    "     <tag type=\"sym\">be</tag>\n",
    "     <tag type=\"lemma\">be</tag>\n",
    "     <tag type=\"from\">1</tag>\n",
    "     <tag type=\"to\">3</tag>\n",
    "     <tag type=\"pos\">VBP</tag>\n",
    "     <tag type=\"sem\">NOW</tag>\n",
    "     <tag type=\"wordnet\">O</tag>\n",
    "   </tags>\n",
    " </tagtoken>\n",
    "\"\"\"\n",
    "\n",
    "test_token = etree.fromstring(test_token_str)\n",
    "print(test_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_pos(token_element):\n",
    "    tags = token_element.find('tags')\n",
    "    tag_dict = {}\n",
    "    for tag in tags:\n",
    "        tag_dict[tag.get(\"type\")] = tag.text\n",
    "    return tag_dict['tok'], tag_dict['pos']\n",
    "\n",
    "# Test your function using the first token \n",
    "token, pos = get_token_pos(test_token)\n",
    "assert token == \"'m\" and pos == 'VBP', 'token and pos not correct'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c Get document text\n",
    "\n",
    "Define a function with the following requirements:\n",
    "\n",
    "* Positional argument: filepath of a document in a particular language (i.e. full, relativ filepath)\n",
    "* Output: the text of the document as a string\n",
    "\n",
    "**Hints**:\n",
    " \n",
    "* `<!-- -->` indicates a comment in an xml file. You cannot access it via etree. Instead, you will have to use the tokens. \n",
    "* You can use a function you have defined in the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, None]\n",
      "[]\n",
      "\n",
      "I 'm not tired at~all .\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "doc text not correct",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-45f387870228>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I 'm not tired at~all .\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"I 'm not tired at~all .\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'doc text not correct'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: doc text not correct"
     ]
    }
   ],
   "source": [
    "def get_doc_text(path_to_doc):\n",
    "    etree.parse(path_to_doc)\n",
    "    tokens = etree.parse(path_to_doc).findall(\"xdrs/taggedtokens/tagtoken\")\n",
    "    return_string = \"\"\n",
    "    for token in tokens:\n",
    "        for tag in token.find(\"tags\"):\n",
    "            if tag.get(\"type\") == \"tok\":\n",
    "                return_string = f\"{return_string} {tag.text}\"\n",
    "    return \n",
    "    \n",
    "#     print(return_string)\n",
    "#     return return_string\n",
    "        \n",
    "\n",
    "# Test you function\n",
    "test_part = 'p27'\n",
    "test_document = 'd0852'\n",
    "language = 'en'\n",
    "test_doc_path = f'{path_pmb}/{test_part}/{test_document}/{language}.drs.xml'\n",
    "\n",
    "text = get_doc_text(test_doc_path)\n",
    "print(text)\n",
    "print(\"I 'm not tired at~all .\")\n",
    "\n",
    "assert text == \"I 'm not tired at~all .\", 'doc text not correct'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.d Sort documents on languages \n",
    "\n",
    "To explore the coverage of the corpus, it is convenient to store the documents as sets mapped to their language. We can then easily use set methods (i.e. intersection) to check which documents exist in a given language pair. \n",
    "\n",
    "Write a function which fulfills the following criteria:\n",
    "\n",
    "* mandatory positional argument: path to the corpus (e.g. '../../../Data/PMB/pmb-2.1.0/data/gold')\n",
    "* output: a dictionary of the following format:\n",
    "            `{\n",
    "              'language1': {'path_to_doc1', 'path_to_doc2', ...},\n",
    "              'language2': {'path_to_doc1', 'path_to_doc4', ...},\n",
    "              'language3': {'path_to_doc2', 'path_to_doc3', ...},\n",
    "              }`\n",
    "       \n",
    "       \n",
    "Hints:\n",
    "\n",
    "* filepaths are strings; you can manipulate them using string methods (e.g. split on '/' or '.'). \n",
    "* The os mudule has a convenient way of extracting the filename from a long path (i.e. the last bit of the path): os.path.basename(your_path)\n",
    "* Feel free to use [defaultdict](https://docs.python.org/3/library/collections.html#collections.defaultdict) (with a set as the default value) (`from collections import defaultdict`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for filepath manipulation:\n",
    "import os \n",
    "\n",
    "my_path = '../../some/dir/with/a/file.txt'\n",
    "f_name = os.path.basename(my_path)\n",
    "print(f_name)\n",
    "remaining_path = my_path.rstrip(f_name)\n",
    "print(remaining_path)\n",
    "name, extension = f_name.split('.')\n",
    "print(name, extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_documents(path_pmb):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test you function:\n",
    "language_doc_dict = sort_docs(path_pmb)\n",
    "\n",
    "n_en = len(language_doc_dict['en'])\n",
    "n_it = len(language_doc_dict['it'])\n",
    "n_de = len(language_doc_dict['de'])\n",
    "n_nl = len(language_doc_dict['nl'])\n",
    "\n",
    "assert n_en == 4555 and n_it == 635 and n_de == 1175 and n_nl == 586, 'sorting not correct'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Putting the tool together\n",
    "\n",
    "Congratulations! You've written most of the code already! \n",
    "\n",
    "From now on, we will mostly use the functions defined above and combine them in the file called `explore_pmb.py`. \n",
    "\n",
    "Some code snippets are provided here to help you along the way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Printing statistics for all languages\n",
    "\n",
    "Let's start by exploring the coverage of all languages individually. In `explore_pmb.py`, write the following code:\n",
    "\n",
    "* Import the function`sort_docs`, call it and assign the output dictionary to a variable called `language_doc_dict`. Don't forget to define the path to the corpus, which you use as function input. \n",
    "* Create a list of all lanugages (hint: you can simply use the keys of `language_doc_dict`). \n",
    "* For each lanugage, print the following:\n",
    "    `[Language]: num docs: [number of documents], num tokens: [number of tokens]\n",
    "    \n",
    "Hints:\n",
    "\n",
    "* Each document is unique - you can simply count the elements in the sets to get the number of documents.\n",
    "* Use the function `get_tokens` to access the tokens. Then count them.\n",
    "* You will most likly use two nested loops: An outer loop for languages and an inner loop to access the tokens in the documents. \n",
    "* Use f-strings to print the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some code to help you along the way (you can also start from scratch)\n",
    "languages = # your code here\n",
    "\n",
    "for languagage in languages:\n",
    "    n_docs = # your code here\n",
    "    docs = # your code here\n",
    "    # your code here\n",
    "    for doc in docs:\n",
    "        path_doc = f'{doc}/{language}.drs.xml\n",
    "        tokens = get_tokens(path_to_doc)\n",
    "        # your code here\n",
    "    print(f'{language}: num docs: {n_docs}, num tokens: {n_tokens}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b Printing statistics for language pairs \n",
    "\n",
    "We also want to explore the coverage of **parallel data** for the lanugage pairs in the corpus. To do this, use an additional loop to iterate over all possible lanugage pairs in the parallal meaning bank and print the number of documents which exist for both languages. \n",
    "\n",
    "Use the function below to generate the lanugage pairs. Simply copy-paste it into the script called `utils.py` and import it into `explore_pmb.py`. Use the cell below to explore how it works. \n",
    "\n",
    "Print the following for each lanugage pair (use f-strings):\n",
    "\n",
    "`Coverage for parallel data in [language_1] and [language_2]: [number of documents]`\n",
    "\n",
    "\n",
    "Hints:\n",
    "\n",
    "* You can unpack tuples in a loop.\n",
    "* Use a set method to get the document paths covered by both languages. Then simply count the paths.\n",
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(language_list):\n",
    "    \"\"\"Given a list, return a list of tuples of all element pairs.\"\"\"\n",
    "    pairs = []\n",
    "    for l1 in language_list:\n",
    "        for l2 in language_list:\n",
    "            if l1 != l2:\n",
    "                if (l1, l2) not in pairs and (l2, l1) not in pairs:\n",
    "                    pairs.append((l1, l2))\n",
    "    return pairs\n",
    "\n",
    "language_list = ['a', 'b', 'c']\n",
    "pairs = get_pairs(language_list)\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.c Explore parallel documents \n",
    "\n",
    "As a final step, we want let the user browse the parallel documents in a chosen language pair. Write the following code (in `explore_pmb.py`):\n",
    "\n",
    "* use input() to define two variables: language_1 and language_2\n",
    "* get the document paths for all documents covered by both languages\n",
    "* Loop over the documents and print the documents in both lanugages. After each document, ask the user whether they want to continue. If the answer is 'no', stop. Else, show the next. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Come up with your own comparison\n",
    "\n",
    "Got insterested in parallel data? Extract a comparison you find interesting! \n",
    "\n",
    "**This is an additional exercise - it is not required to complete this to get a full score.** \n",
    "\n",
    "If you complete this exercise, you can earn up to 3 additional points which can be used to make up for other points you missed. Note that you cannot get more than a full score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing & submission\n",
    "\n",
    "Congratulations! You've built a corpus exploration tool! Before you submit, please make sure to test your code:\n",
    "\n",
    "* Can you run the script `explore_pmb.py` from the command line?\n",
    "* Do you get a general corpus overview (see 3.a)?\n",
    "* Do you get an overview of language pairs (see 3.b)?\n",
    "* Are you asked to provide a lanugage pair and do you see examples of parallel documents once you entered a pair (see 3.c?)\n",
    "\n",
    "If you did not manage to complete all of the exercises, submit what you have and, if possible, explain how you were going to solve them. You may get points for intermediate steps. \n",
    "\n",
    "**Please submit python scripts only. You can use this notebook for exploration and development, but we will not consider the code written here.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
